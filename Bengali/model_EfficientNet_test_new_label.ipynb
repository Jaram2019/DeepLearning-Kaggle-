{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D, GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"../data/bengali_AI_handwritten_grapheme_classification\"\n",
    "\n",
    "IMG_HEIGHT = 137\n",
    "IMG_WIDTH = 236\n",
    "\n",
    "COMBINED_LABEL_NUM = 1292\n",
    "ROOT_CLASSES_NUM = 168\n",
    "CONSONANT_CLASSES_NUM = 7\n",
    "VOWEL_CLASSES_NUM = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_df = pd.read_csv(\"{}/train.csv\".format(DATA_DIR_PATH))\n",
    "test_df = pd.read_csv(\"{}/test.csv\".format(DATA_DIR_PATH))\n",
    "class_map_df = pd.read_csv(\"{}/class_map.csv\".format(DATA_DIR_PATH))\n",
    "sample_submission_df = pd.read_csv(\"{}/sample_submission.csv\".format(DATA_DIR_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_img(img_data):\n",
    "    if isinstance(img_data, pd.Series):\n",
    "        img_data = img_data.to_numpy()\n",
    "    \n",
    "    return img_data.reshape(IMG_HEIGHT, IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTERED_IMAGE_PATH = \"../data/bengali_centered\"\n",
    "PREPROCESSED_TRIAN_DATA_CSV_NAME = \"new_train_data_with_new_label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_with_image_path(df: pd.DataFrame, dataset_name_list: list):\n",
    "    image_path_list = list()\n",
    "\n",
    "    for dataset_name in dataset_name_list:\n",
    "        image_path_data_list = list()\n",
    "        centered_image_dataset_path = os.path.join(CENTERED_IMAGE_PATH, dataset_name)\n",
    "\n",
    "        for file in sorted(os.listdir(centered_image_dataset_path)):\n",
    "            image_id, ext = os.path.splitext(file)\n",
    "\n",
    "            if ext != \".jpg\":\n",
    "                continue\n",
    "\n",
    "            idx = int(image_id.split(\"_\")[-1])\n",
    "            path_data = (idx, image_id, os.path.join(centered_image_dataset_path, file))\n",
    "\n",
    "            image_path_data_list.append(path_data)\n",
    "\n",
    "        image_path_data_list = sorted(image_path_data_list, key=lambda path_data: path_data[0])\n",
    "        image_path_list_in_dataset = list(list(zip(*image_path_data_list))[2])\n",
    "        image_path_list += image_path_list_in_dataset\n",
    "\n",
    "    df['preprocessed_image_path'] = image_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_df_with_image_path(df=original_train_df, dataset_name_list=[\"dataset_1\", \"dataset_2\", \"dataset_3\", \"dataset_4\"])\n",
    "# original_train_df.to_csv(os.path.join(CENTERED_IMAGE_PATH, PREPROCESSED_TRIAN_DATA_CSV_NAME), mode='w', index=False)\n",
    "\n",
    "preprocessed_train_df = pd.read_csv(os.path.join(CENTERED_IMAGE_PATH, PREPROCESSED_TRIAN_DATA_CSV_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>preprocessed_image_path</th>\n",
       "      <th>combined_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200835</th>\n",
       "      <td>200835</td>\n",
       "      <td>Train_200835</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>র্খে</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>22072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200836</th>\n",
       "      <td>200836</td>\n",
       "      <td>Train_200836</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্তো</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>65090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200837</th>\n",
       "      <td>200837</td>\n",
       "      <td>Train_200837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>অ্যা</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200838</th>\n",
       "      <td>200838</td>\n",
       "      <td>Train_200838</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>স্নো</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>152090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200839</th>\n",
       "      <td>200839</td>\n",
       "      <td>Train_200839</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্টি</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>127020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      image_id  grapheme_root  vowel_diacritic  \\\n",
       "200835      200835  Train_200835             22                7   \n",
       "200836      200836  Train_200836             65                9   \n",
       "200837      200837  Train_200837              2                1   \n",
       "200838      200838  Train_200838            152                9   \n",
       "200839      200839  Train_200839            127                2   \n",
       "\n",
       "        consonant_diacritic grapheme  \\\n",
       "200835                    2     র্খে   \n",
       "200836                    0     ত্তো   \n",
       "200837                    4     অ্যা   \n",
       "200838                    0     স্নো   \n",
       "200839                    0     ল্টি   \n",
       "\n",
       "                                  preprocessed_image_path  combined_word  \n",
       "200835  ../data/bengali_centered/dataset_4/Train_20083...          22072  \n",
       "200836  ../data/bengali_centered/dataset_4/Train_20083...          65090  \n",
       "200837  ../data/bengali_centered/dataset_4/Train_20083...           2014  \n",
       "200838  ../data/bengali_centered/dataset_4/Train_20083...         152090  \n",
       "200839  ../data/bengali_centered/dataset_4/Train_20083...         127020  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(preprocessed_train_df['preprocessed_image_path'][2700])\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>preprocessed_image_path</th>\n",
       "      <th>combined_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_0.jpg</td>\n",
       "      <td>15095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_1.jpg</td>\n",
       "      <td>159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_2.jpg</td>\n",
       "      <td>22035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_3.jpg</td>\n",
       "      <td>53022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_4.jpg</td>\n",
       "      <td>71095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200835</th>\n",
       "      <td>200835</td>\n",
       "      <td>Train_200835</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>র্খে</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>22072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200836</th>\n",
       "      <td>200836</td>\n",
       "      <td>Train_200836</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্তো</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>65090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200837</th>\n",
       "      <td>200837</td>\n",
       "      <td>Train_200837</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>অ্যা</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200838</th>\n",
       "      <td>200838</td>\n",
       "      <td>Train_200838</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>স্নো</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>152090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200839</th>\n",
       "      <td>200839</td>\n",
       "      <td>Train_200839</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্টি</td>\n",
       "      <td>../data/bengali_centered/dataset_4/Train_20083...</td>\n",
       "      <td>127020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200840 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      image_id  grapheme_root  vowel_diacritic  \\\n",
       "0                0       Train_0             15                9   \n",
       "1                1       Train_1            159                0   \n",
       "2                2       Train_2             22                3   \n",
       "3                3       Train_3             53                2   \n",
       "4                4       Train_4             71                9   \n",
       "...            ...           ...            ...              ...   \n",
       "200835      200835  Train_200835             22                7   \n",
       "200836      200836  Train_200836             65                9   \n",
       "200837      200837  Train_200837              2                1   \n",
       "200838      200838  Train_200838            152                9   \n",
       "200839      200839  Train_200839            127                2   \n",
       "\n",
       "        consonant_diacritic grapheme  \\\n",
       "0                         5   ক্ট্রো   \n",
       "1                         0        হ   \n",
       "2                         5     খ্রী   \n",
       "3                         2     র্টি   \n",
       "4                         5     থ্রো   \n",
       "...                     ...      ...   \n",
       "200835                    2     র্খে   \n",
       "200836                    0     ত্তো   \n",
       "200837                    4     অ্যা   \n",
       "200838                    0     স্নো   \n",
       "200839                    0     ল্টি   \n",
       "\n",
       "                                  preprocessed_image_path  combined_word  \n",
       "0          ../data/bengali_centered/dataset_1/Train_0.jpg          15095  \n",
       "1          ../data/bengali_centered/dataset_1/Train_1.jpg         159000  \n",
       "2          ../data/bengali_centered/dataset_1/Train_2.jpg          22035  \n",
       "3          ../data/bengali_centered/dataset_1/Train_3.jpg          53022  \n",
       "4          ../data/bengali_centered/dataset_1/Train_4.jpg          71095  \n",
       "...                                                   ...            ...  \n",
       "200835  ../data/bengali_centered/dataset_4/Train_20083...          22072  \n",
       "200836  ../data/bengali_centered/dataset_4/Train_20083...          65090  \n",
       "200837  ../data/bengali_centered/dataset_4/Train_20083...           2014  \n",
       "200838  ../data/bengali_centered/dataset_4/Train_20083...         152090  \n",
       "200839  ../data/bengali_centered/dataset_4/Train_20083...         127020  \n",
       "\n",
       "[200840 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_train_test_split(full_len: int, test_ratio: float) -> tuple:\n",
    "    test_len = int(full_len * test_ratio)\n",
    "    train_len = full_len - test_len\n",
    "    return train_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_dataset(dataset) -> int:\n",
    "    count = 0\n",
    "    for data in dataset:\n",
    "        count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = preprocessed_train_df['preprocessed_image_path']\n",
    "label_list = preprocessed_train_df['combined_word']\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 140588, Val size : 18076, Test size : 42176\n"
     ]
    }
   ],
   "source": [
    "len_train, len_test_full = get_len_train_test_split(full_len=len(path_list), test_ratio=0.3)\n",
    "len_val, len_test = get_len_train_test_split(full_len=len_test_full, test_ratio=0.7)\n",
    "\n",
    "print(f\"Train size : {len_train}, Val size : {len_val}, Test size : {len_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(path_list))\n",
    "print(type(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((path_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.take(len_train)\n",
    "test_dataset = dataset.skip(len_train)\n",
    "val_dataset = test_dataset.take(len_val)\n",
    "test_dataset = test_dataset.skip(len_val)\n",
    "\n",
    "#print(f\"Train size : {get_len_dataset(train_dataset)}, Val size : {get_len_dataset(val_dataset)}, Test size : {get_len_dataset(test_dataset)}\")\n",
    "# Train size : 140588, Val size : 18076, Test size : 42176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_net = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_net.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name='img')\n",
    "\n",
    "m = base_net(inputs)\n",
    "m = GlobalMaxPooling2D()(m)\n",
    "\n",
    "\n",
    "# m = Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "# m = BatchNormalization(momentum=0.15)(m)\n",
    "# m = MaxPool2D(pool_size=(2, 2))(m)\n",
    "# m = Dropout(rate=0.3)(m)\n",
    "\n",
    "# m = Conv2D(64, 3, padding='same', activation='relu')(m)\n",
    "# m = BatchNormalization(momentum=0.15)(m)\n",
    "# m = MaxPool2D(pool_size=(2, 2))(m)\n",
    "# m = Dropout(rate=0.3)(m)\n",
    "\n",
    "# m = Conv2D(128, 3, padding='same', activation='relu')(m)\n",
    "# m = BatchNormalization(momentum=0.15)(m)\n",
    "# m = MaxPool2D(pool_size=(2, 2))(m)\n",
    "# m = Dropout(rate=0.3)(m)\n",
    "\n",
    "# m = Conv2D(256, 3, padding='same', activation='relu')(m)\n",
    "# m = BatchNormalization(momentum=0.15)(m)\n",
    "# m = MaxPool2D(pool_size=(2, 2))(m)\n",
    "# m = Dropout(rate=0.3)(m)\n",
    "\n",
    "\n",
    "# m = Conv2D(512, 3, padding='same', activation='relu')(m)\n",
    "# m = BatchNormalization(momentum=0.15)(m)\n",
    "# m = MaxPool2D(pool_size=(2, 2))(m)\n",
    "# m = Dropout(rate=0.3)(m)\n",
    "\n",
    "# m = Flatten()(m)\n",
    "# m = Dropout(rate=0.2)(m)\n",
    "\n",
    "m = Dense(256, activation='relu')(m)\n",
    "m = Dropout(rate=0.3)(m)\n",
    "m = Dense(128, activation='relu')(m)\n",
    "m = Dropout(rate=0.2)(m)\n",
    "\n",
    "output = Dense(COMBINED_LABEL_NUM, activation='softmax', name='output')(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "img (InputLayer)             [(None, 137, 236, 3)]     0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b0 (Model)      (None, 5, 8, 1280)        4049564   \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               327936    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1292)              166668    \n",
      "=================================================================\n",
      "Total params: 4,577,064\n",
      "Trainable params: 527,500\n",
      "Non-trainable params: 4,049,564\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(base_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_img(path_list):\n",
    "    return tf.map_fn(read_img, path_list, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch : 0 ==\n",
      "\n",
      "Elapsed time for loading image data : 0.9926254749298096\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 4s 3ms/sample - loss: 21755899688.0000 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 0.9443461894989014\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 245811088179.2000 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 0.970935583114624\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 940575725158.3999 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 1.0294115543365479\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 2145295666380.7998 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 1.0240938663482666\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 3857806675148.8003 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 1.0188953876495361\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 6488109350912.0000 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 0.9997057914733887\n",
      "Train on 1280 samples\n",
      "1280/1280 [==============================] - 2s 1ms/sample - loss: 9717208186880.0000 - accuracy: 0.0000e+00\n",
      "Elapsed time for loading image data : 1.2034833431243896\n",
      "Train on 1280 samples\n",
      "  32/1280 [..............................] - ETA: 1s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ee1cd3ec2931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m  \u001b[0;31m# 64, 128, 256 은 GPU 메모리 부족\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_list = list()\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    print(\"== Epoch : {} ==\\n\".format(epoch))\n",
    "    dataset = dataset.shuffle(len(path_list))\n",
    "    \n",
    "    for path_batch, label_batch in train_dataset.batch(1280):\n",
    "#     for path_batch, root_batch, consonant_batch, vowel_batch in train_dataset.batch(1024).take(2):\n",
    "        time_st = time.time()\n",
    "        img_batch = tf.map_fn(read_img, path_batch, dtype=tf.float32)\n",
    "        time_ed = time.time()\n",
    "        print(\"Elapsed time for loading image data : {}\".format(time_ed - time_st))\n",
    "        \n",
    "        history = model.fit(\n",
    "            generator\n",
    "            x={'img': img_batch},\n",
    "            y={'output': label_batch},\n",
    "            batch_size=32  # 64, 128, 256 은 GPU 메모리 부족\n",
    "        )\n",
    "        \n",
    "        # Appending history\n",
    "        history_list.append((epoch, history))\n",
    "    \n",
    "    print(f\"Epoch {epoch} - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
