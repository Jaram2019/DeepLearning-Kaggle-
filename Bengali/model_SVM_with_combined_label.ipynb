{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm.notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D, GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALED_IMAGES_PATH = \"../data/scaled_images\"\n",
    "PREPROCESSED_TRIAN_DATA_CSV_NAME = \"new_train_data_with_new_label.csv\"\n",
    "\n",
    "IMG_HEIGHT = 137\n",
    "IMG_WIDTH = 236\n",
    "\n",
    "COMBINED_LABEL_NUM = 1292\n",
    "ROOT_CLASSES_NUM = 168\n",
    "CONSONANT_CLASSES_NUM = 7\n",
    "VOWEL_CLASSES_NUM = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_img(img_data):\n",
    "    if isinstance(img_data, pd.Series):\n",
    "        img_data = img_data.to_numpy()\n",
    "    \n",
    "    return img_data.reshape(IMG_HEIGHT, IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_df = pd.read_csv(\"../data/bengali_centered/\" + PREPROCESSED_TRIAN_DATA_CSV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>preprocessed_image_path</th>\n",
       "      <th>combined_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_0.jpg</td>\n",
       "      <td>15095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_1.jpg</td>\n",
       "      <td>159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_2.jpg</td>\n",
       "      <td>22035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_3.jpg</td>\n",
       "      <td>53022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>../data/bengali_centered/dataset_1/Train_4.jpg</td>\n",
       "      <td>71095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 image_id  grapheme_root  vowel_diacritic  consonant_diacritic  \\\n",
       "0           0  Train_0             15                9                    5   \n",
       "1           1  Train_1            159                0                    0   \n",
       "2           2  Train_2             22                3                    5   \n",
       "3           3  Train_3             53                2                    2   \n",
       "4           4  Train_4             71                9                    5   \n",
       "\n",
       "  grapheme                         preprocessed_image_path  combined_word  \n",
       "0   ক্ট্রো  ../data/bengali_centered/dataset_1/Train_0.jpg          15095  \n",
       "1        হ  ../data/bengali_centered/dataset_1/Train_1.jpg         159000  \n",
       "2     খ্রী  ../data/bengali_centered/dataset_1/Train_2.jpg          22035  \n",
       "3     র্টি  ../data/bengali_centered/dataset_1/Train_3.jpg          53022  \n",
       "4     থ্রো  ../data/bengali_centered/dataset_1/Train_4.jpg          71095  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(preprocessed_train_df['combined_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd072bc29554e67a1899c1b4d21019c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 30min 58s, sys: 1.17 s, total: 1h 30min 59s\n",
      "Wall time: 1h 31min 1s\n",
      "CPU times: user 1h 30min 51s, sys: 1.26 s, total: 1h 30min 52s\n",
      "Wall time: 1h 30min 54s\n",
      "CPU times: user 1h 30min 29s, sys: 792 ms, total: 1h 30min 30s\n",
      "Wall time: 1h 30min 32s\n",
      "CPU times: user 1h 31min 5s, sys: 1.62 s, total: 1h 31min 7s\n",
      "Wall time: 1h 31min 9s\n",
      "CPU times: user 1h 31min 21s, sys: 1.54 s, total: 1h 31min 23s\n",
      "Wall time: 1h 31min 25s\n",
      "CPU times: user 1h 29min 26s, sys: 1.61 s, total: 1h 29min 28s\n",
      "Wall time: 1h 29min 30s\n",
      "CPU times: user 1h 31min 8s, sys: 1.73 s, total: 1h 31min 10s\n",
      "Wall time: 1h 31min 12s\n",
      "CPU times: user 1h 31min 9s, sys: 1.55 s, total: 1h 31min 10s\n",
      "Wall time: 1h 31min 12s\n",
      "CPU times: user 1h 31min 26s, sys: 1.76 s, total: 1h 31min 28s\n",
      "Wall time: 1h 31min 30s\n",
      "CPU times: user 1h 31min 1s, sys: 1.55 s, total: 1h 31min 3s\n",
      "Wall time: 1h 31min 5s\n",
      "CPU times: user 1h 30min 19s, sys: 1.65 s, total: 1h 30min 20s\n",
      "Wall time: 1h 30min 22s\n",
      "CPU times: user 1h 30min 47s, sys: 1.62 s, total: 1h 30min 49s\n",
      "Wall time: 1h 30min 51s\n",
      "CPU times: user 1h 30min 59s, sys: 1.72 s, total: 1h 31min\n",
      "Wall time: 1h 31min 2s\n",
      "CPU times: user 1h 30min 57s, sys: 1.65 s, total: 1h 30min 59s\n",
      "Wall time: 1h 31min 1s\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a194f0adb341b0841cf3aa0f146b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "test_scaled_img_list = list()\n",
    "for epoch in range(50):\n",
    "    print(\"Epoch : \", epoch)\n",
    "    for i in tqdm.tqdm(range(14)): # train data\n",
    "        with open(SCALED_IMAGES_PATH + f\"/scaled_img_{i}.pkl\", \"rb\") as f:\n",
    "            scaled_imgs = pickle.load(f)\n",
    "\n",
    "        %time svc.fit(scaled_imgs, labels[i*dataset_size:(i+1)*dataset_size])\n",
    "        del scaled_imgs\n",
    "        \n",
    "    # check accuracy with test data\n",
    "    for j in tqdm.tqdm(range(14, 20)):\n",
    "        with open(SCALED_IMAGES_PATH + f\"/scaled_img_{i}.pkl\", \"rb\") as f:\n",
    "            scaled_imgs = pickle.load(f)\n",
    "\n",
    "        test_scaled_img_list.append(scaled_imgs)\n",
    "        del scaled_imgs\n",
    "\n",
    "    test_scaled_img_list = np.array(test_scaled_img_list)\n",
    "    test_scaled_img_list = test_scaled_img_list.squeeze(0)\n",
    "\n",
    "    pred = svc.predict(test_scaled_img_list)\n",
    "    acc = accuracy_score(labels[14*dataset_size:20*dataset_size], pred)\n",
    "    print(\"Acc : \", acc)\n",
    "    del test_scaled_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_scaled_img_list = list()\n",
    "for epoch in range(1):\n",
    "    print(\"Epoch : \", epoch)\n",
    "    for i in range(0, 1): # train data\n",
    "        with open(SCALED_IMAGES_PATH + f\"/scaled_img_{i}.pkl\", \"rb\") as f:\n",
    "            scaled_imgs = pickle.load(f)\n",
    "\n",
    "        %time svc.fit(scaled_imgs, labels[i*dataset_size:(i+1)*dataset_size])\n",
    "        del scaled_imgs\n",
    "        \n",
    "    # check accuracy with test data\n",
    "    for j in tqdm.tqdm(range(1, 2)):\n",
    "        with open(SCALED_IMAGES_PATH + f\"/scaled_img_{i}.pkl\", \"rb\") as f:\n",
    "            scaled_imgs = pickle.load(f)\n",
    "\n",
    "        test_scaled_img_list.append(scaled_imgs)\n",
    "        del scaled_imgs\n",
    "\n",
    "    test_scaled_img_list = np.array(test_scaled_img_list)\n",
    "    test_scaled_img_list = test_scaled_img_list.squeeze(0)\n",
    "\n",
    "    pred = svc.predict(test_scaled_img_list)\n",
    "    acc = accuracy_score(labels[1*dataset_size:2*dataset_size], pred)\n",
    "    print(\"Acc : \", acc)\n",
    "    del test_scaled_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm.tqdm(range(14, 20)):\n",
    "    with open(SCALED_IMAGES_PATH + f\"/scaled_img_{i}.pkl\", \"rb\") as f:\n",
    "        scaled_imgs = pickle.load(f)\n",
    "\n",
    "    test_scaled_img_list.append(scaled_imgs)\n",
    "    del scaled_imgs\n",
    "\n",
    "test_scaled_img_list = np.array(test_scaled_img_list)\n",
    "test_scaled_img_list = test_scaled_img_list.squeeze(0)\n",
    "\n",
    "pred = svc.predict(test_scaled_img_list)\n",
    "acc = accuracy_score(labels[14*dataset_size:20*dataset_size], pred)\n",
    "print(\"Acc : \", acc)\n",
    "del test_scaled_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.asarray(test_scaled_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10042, 32332)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_temp = svc.predict(temp)\n",
    "acc_temp = accuracy_score(labels[14*dataset_size:15*dataset_size], pred_temp)\n",
    "print(\"Acc : \", acc_temp)\n",
    "# del test_scaled_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_img_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [64, 64])\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_img(path_list):\n",
    "    return tf.map_fn(read_img, path_list, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((path_list, root_list, consonant_list, vowel_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list = list()\n",
    "for epoch in range(50):\n",
    "    \n",
    "    print(\"== Epoch : {} ==\\n\".format(epoch))\n",
    "#     dataset = dataset.shuffle(len(path_list))\n",
    "    \n",
    "#     for path_batch, root_batch, consonant_batch, vowel_batch in train_dataset.batch(25600):\n",
    "#         time_st = time.time()\n",
    "#         img_batch = tf.map_fn(read_img, path_batch, dtype=tf.float32)\n",
    "#         time_ed = time.time()\n",
    "#         print(\"Elapsed time for loading image data : {}\".format(time_ed - time_st))\n",
    "        \n",
    "        history = svc.fit(img_batch, label_batch)\n",
    "        \n",
    "        # Appending history\n",
    "        history_list.append((epoch, history))\n",
    "    \n",
    "    print(f\"Epoch {epoch} - \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
